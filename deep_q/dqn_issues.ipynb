{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- I'll rapidly go over the changes i made to the agent and environment to accomodate the DQN approach\n",
    "    - model taken from <a href=\"https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\">pytorch's site<a/>\n",
    "    - the invalid action cost in the environment is now set to -0.01, for the following reason: in order to follow the convention to choose the maximum action-value, in the TSP context this would convert to the 1/reward (where the reward is the distance between nodes). A bigger distance results to a lower action value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdeep_q\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcustom_env_deep_q\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TSPEnv\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mTSPDistCost\u001B[39;00m(TSPEnv):\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;66;03m# (...)\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
      "File \u001B[1;32m~\\Desktop\\Erasmus UPM\\Master Thesis\\git_new\\deep_q\\custom_env_deep_q.py:3\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m spaces\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mor_gym\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n",
      "File \u001B[1;32mc:\\users\\housh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\__init__.py:119\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    117\u001B[0m is_loaded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_load_library_flags:\n\u001B[1;32m--> 119\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mkernel32\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadLibraryExW\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdll\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0x00001100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m     last_error \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mget_last_error()\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m last_error \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m126\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from deep_q.custom_env_deep_q import TSPEnv\n",
    "\n",
    "\n",
    "class TSPDistCost(TSPEnv):\n",
    "    # (...)\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.N = 7\n",
    "\n",
    "        self.invalid_action_cost = -0.01\n",
    "    # (...)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- This also happens in the step process, where the reward is again defined as the inverse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    def _STEP(self, action):\n",
    "        done = False\n",
    "\n",
    "        #check if the action chosen is the starting one\n",
    "        #if action == self.start_node and self.step_count<(self.N + 1):\n",
    "\n",
    "        if self.visit_log[action] > 0:\n",
    "            # Node already visited\n",
    "            #changed for DQN\n",
    "            reward = 1/self.invalid_action_cost\n",
    "            done = True\n",
    "        else:\n",
    "            #changed for DQN\n",
    "            if(self.current_node == action):\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = 1 / self.distance_matrix[self.current_node, action]\n",
    "            self.current_node = action\n",
    "            self.visit_log[self.current_node] = 1\n",
    "\n",
    "        self.state = self._update_state()\n",
    "        # See if all nodes have been visited\n",
    "        unique_visits = self.visit_log.sum()\n",
    "\n",
    "        if unique_visits == self.N:\n",
    "            done = True\n",
    "\n",
    "        return self.state, reward, done, {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DQN\n",
    "- The DQN is defined as follows, please tell me if it makes sense\n",
    "    - it takes as input the state and outputs a tensor of action values, to determine the best action to take (in theory the one with the highest value). The index of the action_value corresponds to the action (node chosen) to make.\n",
    "    - the initial epsilon is 0.2 here, but i changed it around up to 0.5 to see the effects on the learning process. 0.2 seemed to be optimal.\n",
    "    - I tried ReLu, tanh, softmax and sigmoid just to see the effects, for now RReLu seemed to be the best performing activation function. I'll try more combinations later on, with a more rational approach and not just by trial and error.\n",
    "    - The minibatch size at the beginning was 64, but the learning didnt look to be going at a satisfying rate. I changed it to 16, but I suppose i'll have to push it back to 64 as the number of nodes gets higher."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_nodes):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.n_actions = n_nodes\n",
    "        self.gamma = 0.99\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.initial_epsilon = 0.2\n",
    "        self.decay_epsilon = 7000\n",
    "        self.num_iterations = 2000000\n",
    "        self.replay_mem_size = 10000\n",
    "        self.minibatch_size = 16\n",
    "        #inplace=True means that it will modify the input directly, without allocating any additional output.\n",
    "        self.relu1 = nn.RReLU(inplace=True)\n",
    "\n",
    "        self.relu2 = nn.RReLU(inplace=True)\n",
    "\n",
    "        self.relu3 = nn.RReLU(inplace=True)\n",
    "        self.fc4   = nn.Linear(in_features=8, out_features=512)\n",
    "        self.relu4 = nn.RReLU(inplace = True)\n",
    "        self.fc5 = nn.Linear(in_features=512, out_features=n_nodes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        floats = []\n",
    "        for val in x:\n",
    "            floats.append(val.double())\n",
    "        #then convert from list to tensor\n",
    "        tensFloats = torch.stack(floats)\n",
    "        out = self.relu1(tensFloats)\n",
    "        out = self.relu2(out)\n",
    "        out = self.relu3(out)\n",
    "        out = (out.float())\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Agent\n",
    "The agent is defined as follows\n",
    "- it has a policy and a target network. The policy net determines the actions ot be taken by the agent, it's fed the current state of the environment. I'll have to read up more in depth about the actual usefulness of the target network, I understood it's for stability purposes, but I have yet to understand where and when to put it exactly for future applications.\n",
    "- I thought a replay buffer of 10k experiences (s, a, s', r tuples) was enough for this model, will see if it needs to be exponentially scaled up for bigger applications.\n",
    "- the Optimization algorithm is Adam, have not yet played with other algorithms.\n",
    "(some of the lines here are purely for my future reference for writing down the actual thesis)\n",
    "- It combines the advantages of\n",
    "   - Adaptive Gradient Algorithm :\n",
    "       - Adaptive Gradient Algorithm (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n",
    "   - RMS propagation\n",
    "        - Root Mean Square Propagation, that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n",
    "- Huber Loss for loss calculation\n",
    "    - ref here <a href=\"https://en.wikipedia.org/wiki/Huber_loss\"> wiki page</a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "from agent_deep_q import *\n",
    "import torch\n",
    "from deep_q.replay_memory import Experience, ExperienceBuffer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env : TSPDistCost):\n",
    "        self.env = env\n",
    "        self.exp_buffer = ExperienceBuffer(10000)\n",
    "        self.policy_net = DQN(env.N).to(device)\n",
    "        self.target_net = DQN(env.N).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n",
    "        self.criterion = nn.HuberLoss()\n",
    "        self._reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Action Selection\n",
    "- epsilon greedy policy, will start with the value defined in the agent initialization and will decade over time to reduce the number of random actions taken.\n",
    "- ```python\n",
    "  index_action = torch.argmax(self.policy_net(state))\n",
    "  ```\n",
    "  will pass the state through the NN, estimate the action values and choose the index of the highest action value, which will determine the best action at this point in time.\n",
    "- the random choice will be taken with\n",
    "- ```python\n",
    "    torch.tensor([[random.randint(0,self.env.N -1 )]], device=device, dtype=torch.long)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " import random\n",
    "\n",
    "\n",
    "def select_action(self, state):\n",
    "        global steps_done\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.policy_net.final_epsilon + (self.policy_net.initial_epsilon - self.policy_net.final_epsilon) * math.exp(-1. * steps_done / self.policy_net.decay_epsilon)\n",
    "        steps_done += 1\n",
    "        # if(steps_done%400 == 0):\n",
    "        #     print(f\"{steps_done} --> eps : {eps_threshold}\" )\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # this will extract the index of the action_value with the current highest\n",
    "                #value, ensuring that the action with the highest possible (expected) reward is chosen.\n",
    "                index_action = torch.argmax(self.policy_net(state))\n",
    "                return index_action\n",
    "\n",
    "        else:\n",
    "            return torch.tensor([[random.randint(0,self.env.N -1 )]], device=device, dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Optimization\n",
    "- Probably the most important function, every batch_size steps it extracts a batch of experiences"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "    def optimize_model(self):\n",
    "        if len(self.exp_buffer) < self.policy_net.minibatch_size:\n",
    "            return\n",
    "        transitions = self.exp_buffer.sample(self.policy_net.minibatch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation).\n",
    "        # This converts batch-array of Experiences to Experience of batch-arrays.\n",
    "\n",
    "        batch = Experience(*zip(transitions))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Non final states are masked as False, Final states (None values) are True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.new_state[0])), device=device, dtype=torch.bool)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract non final states, will use a more efficient solution as soon as i am sure about the whole algorithm."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "        tmp_non_final_next_states = []\n",
    "        for val in batch.new_state[0]:\n",
    "            if val is not None:\n",
    "                tmp_non_final_next_states.append(val)\n",
    "\n",
    "        #shape is (batch_size, num_nodes + 1)\n",
    "        non_final_next_states = torch.stack(tmp_non_final_next_states)\n",
    "\n",
    "        #concatenate non final states in the batch in non_final_next_states\n",
    "        state_batch = torch.stack(batch.state[0])\n",
    "\n",
    "        #reshaping to ensure same dimensions as state_action_values\n",
    "        action_batch_reshape = (np.reshape(batch.action[0], (self.policy_net.minibatch_size,1))).astype(np.int64)\n",
    "        action_batch_v2 = torch.as_tensor(action_batch_reshape)\n",
    "\n",
    "        reward_batch = torch.from_numpy(batch.reward[0])\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next line will first feed the batch of states to the network, which will output the action values of the states.\n",
    "To make this a bit clearer, let's take for example a batch of size 2\n",
    "    - [0] = (6,0,0,0,1,0,1,1)\n",
    "    - [1] = (3,0,1,1,1,0,0,0)\n",
    "will be fed to the NN, which will output two tensors of size (n_actions), for example with these values\n",
    "    - [0]     (0.34, 0.79, -0.55, 0.22, 0.11, 0.59)\n",
    "    - [1]     (0.89, -0.98, 0.34, 0.84, 0.32, -0.11)\n",
    "\n",
    "the action_batch will be for example [3, 2] (as these were the actions taken) in the experiences. The state_action values extracted will be at index 3 for row 0, 2 for row 1, so [0.22, 0.34]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch_v2)\n",
    "\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.policy_net.minibatch_size, device=device)\n",
    "\n",
    "        tmp_next_state_values = self.target_net(non_final_next_states)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract max state action values from the next states"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "        max, ind =  torch.max(tmp_next_state_values,dim=1)\n",
    "        next_state_values[non_final_mask] = max\n",
    "        # Compute the expected Q values\n",
    "        reward_batch = torch.FloatTensor(reward_batch)\n",
    "        expected_state_action_values = (next_state_values * self.policy_net.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        #print(loss)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #loss returned purely to see the progress made by the model\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Episode being played"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    from deep_q.replay_memory import Experience\n",
    "\n",
    "\n",
    "def play_episodes(self):\n",
    "        def display_distances(matrix, path):\n",
    "            print(\"Distances in path :\")\n",
    "            tot_dist = 0\n",
    "            for _ in range(1, len(path)):\n",
    "                dist = matrix[path[_]][path[_ - 1]]\n",
    "                tot_dist += dist\n",
    "                print(f\"{path[_ - 1]} --> {path[_]} : {dist}\")\n",
    "            print(f\"Total distance : {tot_dist}\")\n",
    "\n",
    "        num_episodes = 20000\n",
    "        for i_episode in range(num_episodes):\n",
    "            tot_reward = 0\n",
    "            # Initialize the environment and state\n",
    "            self.env.reset()\n",
    "            path = [self.env.state[0].item()]\n",
    "\n",
    "\n",
    "            state = self.env.state\n",
    "            for t in range(self.env.step_limit):#count():\n",
    "                # Select and perform an action\n",
    "                action = self.select_action(state).item()\n",
    "                _, reward, done, _ = self.env.step(action)\n",
    "                path.append(action)\n",
    "                #reward = torch.tensor(reward, device=device)\n",
    "                tot_reward += reward\n",
    "\n",
    "                # Observe new state\n",
    "\n",
    "                if not done:\n",
    "                    next_state = self.env.state\n",
    "                else:\n",
    "                    next_state = None#torch.ones([self.env.N + 1], dtype=torch.int32)*-1\n",
    "\n",
    "                # Store the transition in memory\n",
    "                self.exp_buffer.append(Experience(state, action,reward, done, next_state))\n",
    "\n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "                loss = self.optimize_model()\n",
    "                if done:\n",
    "\n",
    "                    #close loop\n",
    "                    path.append(path[0])\n",
    "                    #print(f\"path {path}: {reward}\")\n",
    "\n",
    "                    break\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if i_episode % 10 == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "            if i_episode % 512 == 0:\n",
    "                print(f\"path {path}: {tot_reward}, loss = {loss}\")\n",
    "                display_distances(self.env.distance_matrix, path)\n",
    "                self.env.render_custom(path)\n",
    "\n",
    "\n",
    "        print('Complete')\n",
    "        #self.env.render_custom(path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Issues so far\n",
    "- after loss goes under 0.4 / 0.3, the NN actually seems to take worse decisions, probably will set it as cut off point or will play around with hyperparameters. why does it behave like this?\n",
    "- the way the environment is set, it doesnt really consider closing the loop. the way i implemented it so far is to calculate the fastest route to traverse all nodes and close the loop by rudimentally connecting last and first node. I assume this isn't exactly the best implementation for TSP, but the or-gym environment is set in the following way for the ending condition"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " unique_visits = self.visit_log.sum()\n",
    "        if unique_visits == self.N:\n",
    "            done = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And when it resets the environment it sets the initial node to 1 (visited)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "self.visit_log[self.current_node] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unless im making a gross mistake this model terminates when all the nodes have been visited (and not for N+1, which would imply a closed path). I could think about another solution and change the current model if you require a closed loop to be considered in the state-action value operation."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}