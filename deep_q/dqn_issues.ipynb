{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- I'll rapidly go over the changes i made to the agent and environment to accomodate the DQN approach\n",
    "    - model taken from <a href=\"https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\">pytorch's site<a/>\n",
    "    - the invalid action cost in the environment is now set to -0.01, for the following reason: in order to follow the convention to choose the maximum action-value, in the TSP context this would convert to the 1/reward (where the reward is the distance between nodes). A bigger distance results to a lower action value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdeep_q\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcustom_env_deep_q\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TSPEnv\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mTSPDistCost\u001B[39;00m(TSPEnv):\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;66;03m# (...)\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
      "File \u001B[1;32m~\\Desktop\\Erasmus UPM\\Master Thesis\\git_new\\deep_q\\custom_env_deep_q.py:3\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m spaces\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mor_gym\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n",
      "File \u001B[1;32mc:\\users\\housh\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\__init__.py:119\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    117\u001B[0m is_loaded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_load_library_flags:\n\u001B[1;32m--> 119\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mkernel32\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLoadLibraryExW\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdll\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0x00001100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m     last_error \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mget_last_error()\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m last_error \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m126\u001B[39m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from deep_q.custom_env_deep_q import TSPEnv\n",
    "\n",
    "\n",
    "class TSPDistCost(TSPEnv):\n",
    "    # (...)\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.N = 7\n",
    "\n",
    "        self.invalid_action_cost = -0.01\n",
    "    # (...)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- This also happens in the step process, where the reward is again defined as the inverse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    def _STEP(self, action):\n",
    "        done = False\n",
    "\n",
    "        #check if the action chosen is the starting one\n",
    "        #if action == self.start_node and self.step_count<(self.N + 1):\n",
    "\n",
    "        if self.visit_log[action] > 0:\n",
    "            # Node already visited\n",
    "            #changed for DQN\n",
    "            reward = 1/self.invalid_action_cost\n",
    "            done = True\n",
    "        else:\n",
    "            #changed for DQN\n",
    "            if(self.current_node == action):\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = 1 / self.distance_matrix[self.current_node, action]\n",
    "            self.current_node = action\n",
    "            self.visit_log[self.current_node] = 1\n",
    "\n",
    "        self.state = self._update_state()\n",
    "        # See if all nodes have been visited\n",
    "        unique_visits = self.visit_log.sum()\n",
    "\n",
    "        if unique_visits == self.N:\n",
    "            done = True\n",
    "\n",
    "        return self.state, reward, done, {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DQN\n",
    "- The DQN is defined as follows, please tell me if it makes sense\n",
    "    - it takes as input the state and outputs a tensor of action values, to determine the best action to take (in theory the one with the highest value). The index of the action_value corresponds to the action (node chosen) to make.\n",
    "    - the initial epsilon is 0.2 here, but i changed it around up to 0.5 to see the"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_nodes):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.n_actions = n_nodes\n",
    "        self.gamma = 0.99\n",
    "        self.final_epsilon = 0.0001\n",
    "        self.initial_epsilon = 0.2\n",
    "        self.decay_epsilon = 7000\n",
    "        self.num_iterations = 2000000\n",
    "        self.replay_mem_size = 10000\n",
    "        self.minibatch_size = 16\n",
    "        #inplace=True means that it will modify the input directly, without allocating any additional output.\n",
    "        self.relu1 = nn.RReLU(inplace=True)\n",
    "\n",
    "        self.relu2 = nn.RReLU(inplace=True)\n",
    "\n",
    "        self.relu3 = nn.RReLU(inplace=True)\n",
    "        self.fc4   = nn.Linear(in_features=8, out_features=512)\n",
    "        self.relu4 = nn.RReLU(inplace = True)\n",
    "        self.fc5 = nn.Linear(in_features=512, out_features=n_nodes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        floats = []\n",
    "        for val in x:\n",
    "            floats.append(val.double())\n",
    "        #then convert from list to tensor\n",
    "        tensFloats = torch.stack(floats)\n",
    "        out = self.relu1(tensFloats)\n",
    "        out = self.relu2(out)\n",
    "        out = self.relu3(out)\n",
    "        out = (out.float())\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.fc5(out)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}